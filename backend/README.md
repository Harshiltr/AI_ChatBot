# AI ChatBot ‚Äì Backend (FastAPI)

This is the backend service for the **AI ChatBot with RAG and local LLM support**.  
It is responsible for authentication, chat handling, document ingestion, vector search, and streaming AI responses.

The backend is built using **FastAPI** and follows a clean, modular structure suitable for production-grade AI applications.

---

## üöÄ Features

- JWT-based user authentication (signup, login, logout)
- User-isolated chat history storage
- Streaming AI responses (token-by-token)
- Retrieval Augmented Generation (RAG)
- Document upload and processing (PDF)
- Vector database integration (ChromaDB)
- Local LLM inference using Ollama
- Database migrations using Alembic

---

## üß† Architecture Overview

- **FastAPI** ‚Äì API framework
- **SQLAlchemy + PostgreSQL** ‚Äì Persistent storage
- **Alembic** ‚Äì Database migrations
- **ChromaDB** ‚Äì Vector database for embeddings
- **Ollama** ‚Äì Local LLM execution (no external API)
- **RAG Pipeline** ‚Äì Combines chat history + document context

All AI inference is performed **locally**, ensuring privacy, zero cost, and no token limits.

---

## ‚öôÔ∏è Setup Instructions

### Prerequisites
- Python 3.10+
- PostgreSQL
- Ollama (https://ollama.com)

---

### 1Ô∏è‚É£ Create virtual environment
```bash 
python -m venv venv
venv\Scripts\activate
```

### 2Ô∏è‚É£ Install dependencies
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Configure environment variables
```bash
Create a .env file using .env.example as reference.
```

### 4Ô∏è‚É£ Run database migrations
```bash
alembic upgrade head
```

### 5Ô∏è‚É£ Start the backend server
```bash
uvicorn app.main:app --reload
```

**Backend URL:**
http://127.0.0.1:8000


**Swagger API docs:**
http://127.0.0.1:8000/docs


----------------------
### ü§ñ Running the LLM (Required)

In a separate terminal:-
```bash
ollama pull llama3
ollama serve
```

**Ollama Backend:-**
http://localhost:11434